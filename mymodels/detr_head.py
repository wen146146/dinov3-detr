import math

import torch
import torch.nn as nn
# ç‰¹å¾å›¾å±•å¹³åä¸¢å¤±äº†ç©ºé—´ä½ç½®ä¿¡æ¯ï¼Œå¿…é¡»æ·»åŠ ä½ç½®ç¼–ç 
class SimpleDETRHead(nn.Module):
    def __init__(self, num_classes=80, num_queries=10,feat_height=14, feat_width=14):#è®¾ç½®å…«åä¸ªç±»åˆ«ï¼Œæœ€å¤šæŸ¥è¯¢100ä¸ªç›®æ ‡
        super().__init__()
        self.num_queries = num_queries #ä¼ é€’ç§ç±»

        # ç‰¹å¾æŠ•å½±
        #é™ç»´ï¼šå°†768ç»´ç‰¹å¾é™åˆ°256ç»´ï¼Œå‡å°‘è®¡ç®—é‡  å·ç§¯æ ¸: 1x1 - åªæ”¹å˜é€šé“æ•°ï¼Œä¸æ”¹å˜ç©ºé—´å°ºå¯¸
        # è¾“å…¥: [batch, 768, H, W]
        # è¾“å‡º: [batch, 256, H, W] ğŸ•µï¸
        self.input_proj = nn.Sequential(
            nn.Conv2d(768, 512, kernel_size=3, padding=1),  # 3Ã—3å·ç§¯
            nn.BatchNorm2d(512),# æ‰¹å½’ä¸€åŒ–
            nn.ReLU(inplace=True),# æ¿€æ´»å‡½æ•°
            nn.Conv2d(512, 256, kernel_size=1),  # å†ç”¨1Ã—1é™ç»´
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )

        # æŸ¥è¯¢å‘é‡
        # ğŸ”¥ ç¬¬1æ­¥ï¼šåˆ›å»º100å¼ éšæœºåŒºåŸŸï¼Œå’Œ256çš„åŒºåŸŸæè¿°åˆ—è¡¨
        self.query_embed = nn.Embedding(num_queries, 256)#è¿™é‡Œçš„256å’Œä¸Šè¿°çš„ç‰¹å¾å€¼æ— å…³
        # ğŸ”¥ ç¬¬2æ­¥ï¼šåˆ›å»º100ä¸ªæœç´¢å™¨ï¼Œå’Œ256çš„æœç´¢å™¨æè¿°åˆ—è¡¨
        self.content_queries = nn.Embedding(num_queries, 256)
        # ğŸ”¥ ç¬¬3æ­¥ï¼šç»™åˆ›å»ºçš„åŒºåŸŸå’Œæœç´¢å™¨éšæœºåˆå§‹åŒ–æƒé‡åˆ—è¡¨ğŸ•µï¸
        self._init_queries(num_queries)


        # Transformerè§£ç å™¨ï¼ˆ6å±‚ï¼‰
        decoder_layer = nn.TransformerDecoderLayer(d_model=256, # d_model=256: ç‰¹å¾ç»´åº¦
                                                   nhead=8,# nhead=8: æ³¨æ„åŠ›å¤´æ•°ï¼ˆ8ä¸ªå¤´å¹¶è¡Œå¤„ç†ï¼‰# æ¯ä¸ªå¤´ç»´åº¦ = 256 / 8 = 32(å…«ä¸ªç»´åº¦ï¼Œæ¯ä¸ªæ³¨æ„32ä¸ªç‰¹å¾ï¼Œå¯ä»¥æ›´ä¸“ä¸š)
                                                   batch_first=True,
                                                   dim_feedforward=2048,  # å¢åŠ FFNç»´åº¦FFNç»´åº¦=2048å°±æ˜¯å°†åŸæœ¬çš„256ä¸ªç‰¹å¾å€¼æ‰©å±•æˆ2048ä¸ªï¼Œç„¶åè¿›è¡Œåˆ†æï¼Œåˆ†æç»“æŸåå†è½¬å›256ï¼Œè¿™æ ·å¯ä»¥ä½¿å¾—åˆ°çš„256ä¸ªç‰¹å¾å€¼æ›´ç²¾å‡†
                                                   dropout=0.1, # æ·»åŠ dropoutå…³é—­0.1çš„ç¥ç»å…ƒé˜²æ­¢è¿‡æ‹Ÿåˆ
                                                   activation = 'gelu'  # âœ“ ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°ï¼ˆæ¯”ReLUæ›´å¹³æ»‘
                                                   )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=4)

        # å±‚å½’ä¸€åŒ–ç»´åº¦è®¾ç½®ï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
        self.norm = nn.LayerNorm(256)

        # é¢„æµ‹å¤´ï¼ˆç±»åˆ«ï¼‰
        self.class_head = nn.Sequential(
            nn.Linear(256, 512),  # âœ“ å¢åŠ ä¸­é—´ç»´åº¦
            nn.LayerNorm(512),  # âœ“ å¯¹åº”æ–°ç»´åº¦
            #nn.Linear(256, 256),  # ç¬¬ä¸€å±‚ï¼šç‰¹å¾æ·±åŒ–ï¼ˆå°†ç‰¹å¾å€¼ä¸­çš„ç‰¹å¾ç»„åˆï¼Œå’Œå»é™¤å™ªå£°ç­‰ï¼Œä½¿å¾—è¡¨ç°æ›´åŠ æ¸…æ™°ï¼‰
            #nn.LayerNorm(256),  # å°†ç‰¹å¾å€¼ç¨³å®šå†ä¸€å®šèŒƒå›´ï¼Œç¨³å®šè®­ç»ƒ
            nn.ReLU(inplace=True),  #å°†è´Ÿç‰¹å¾å€¼å›ºå®šä¸º0 éçº¿æ€§æ¿€æ´»
            nn.Dropout(0.2),  # âœ“ æé«˜dropout
            nn.Linear(512, 256),  # âœ“ å¢åŠ ä¸€å±‚
            nn.LayerNorm(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(256, num_classes + 1)  # æœ€ç»ˆåˆ†ç±»
        )

        # é¢„æµ‹å¤´ï¼ˆåæ ‡ï¼‰
        self.bbox_head = nn.Sequential(
            nn.Linear(256, 512),  # âœ“ å¢åŠ å®¹é‡
            nn.LayerNorm(512),
            #nn.Linear(256, 256),
            #nn.LayerNorm(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.15),  # âœ“ é€‚å½“dropout
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(256, 4)  # [cx, cy, w, h]
        )
        #ç»™æ¯ä¸ªæ¨¡å—å›ºå®šåæ ‡
        self.pos_encoding = LearnablePositionalEncoding2D(256, feat_height, feat_width)

        #è®¾ç½®ä½ç½®å’Œç±»åˆ«çš„æƒé‡ï¼Œå‘Šè¯‰ä»–æ›´åº”è¯¥æ³¨é‡ä½ç½®è¿˜æ˜¯ç‰©ç§
        self.query_fusion = ResidualQueryFusion(d_model=256)

    def forward(self, feature_map):
        #æŸ¥çœ‹dinov3ä¼ è¾“è¿‡æ¥çš„æ•°æ® [batch, 768, 14, 14]
        batch_size = feature_map.shape[0]

        # ç‰¹å¾æŠ•å½±
        # è¾“å…¥: [batch, 768, H, W]
        # è¾“å‡º: [batch, 256, H, W]
        memory = self.input_proj(feature_map)

        #æ·»åŠ 2Dä½ç½®ç¼–ç 
        memory = self.pos_encoding(memory)

        # è½¬æ¢ç»´åº¦
        # å‡è®¾è¾“å…¥: [2, 256, 14, 14]
        memory = (memory.flatten(2).  # [2, 256, 196]  # å°†14Ã—14=196ä¸ªç©ºé—´ä½ç½®å±•å¹³ï¼ˆåƒç´ åæ ‡ä¸¢å¤±ï¼Œéœ€è¦ä½ç½®ç¼–ç ï¼‰
                  permute(0, 2, 1))  # [2, 196, 256]  # äº¤æ¢ç»´åº¦


        # ğŸ”¥ 1. ä½ç½®æŸ¥è¯¢ï¼ˆåœ°å›¾ï¼‰
        query_pos = self.query_embed.weight.unsqueeze(0).repeat(batch_size, 1, 1)
        # ğŸ”¥ 2. å†…å®¹æŸ¥è¯¢ï¼ˆå¤§è„‘ï¼‰
        tgt = self.content_queries.weight.unsqueeze(0).repeat(batch_size, 1, 1)
        # å°†ä½ç½®ä¿¡æ¯åŠ åˆ°å†…å®¹ä¸­


        #tgt_with_pos = tgt + query_pos
        tgt_with_pos =self.query_fusion(tgt,query_pos)

        # è§£ç 
        output = self.decoder(tgt_with_pos, memory)

        # å±‚å½’ä¸€åŒ–
        output = self.norm(output)#å¹³è¯æƒé‡å€¼

        # é¢„æµ‹
        pred_logits = self.class_head(output)  # [batch, 100, 81]  å°†256ä¸ªç‰¹å¾å€¼ï¼Œè½¬åŒ–æˆ81ä¸ªç±»çš„ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”
        pred_boxes = torch.sigmoid(self.bbox_head(output))  # [batch, 100, 4] ç»™å‡ºè¿™100ä¸ªé¢„æµ‹çš„åæ ‡

        return pred_logits, pred_boxes  # è¿”å›é¢„æµ‹çš„ç±»åˆ«å’Œè¾¹ç•Œæ¡†

    def _init_queries(self, num_queries):
        """æ™ºèƒ½åˆå§‹åŒ–æŸ¥è¯¢å‘é‡"""
        #ä»–ç¬¬ä¸€æ¬¡ç”Ÿæˆï¼Œæ˜¯æ¯”è¾ƒæœ‰ç»éªŒçš„åˆ†é…äº†ä¸€ä¸‹ï¼Œä»¥åå¯ä»¥è€ƒè‡ªå·±è®­ç»ƒ
        nn.init.normal_(self.query_embed.weight, mean=0.0, std=0.02)
        # å†…å®¹æŸ¥è¯¢ï¼ˆcontent_queriesï¼‰ï¼šå­¦ä¹ ç‰©ä½“ç‰¹å¾
        # ä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼Œæ ‡å‡†å·®0.02ï¼ˆå°å€¼é¿å…åˆå§‹æ¿€æ´»è¿‡å¤§ï¼‰
        nn.init.normal_(self.content_queries.weight, mean=0.0, std=0.01)


class LearnablePositionalEncoding2D(nn.Module):
    def __init__(self, d_model, height, width):
        super().__init__()
        # å½“å‰ï¼šéšæœºåˆå§‹åŒ–
        # self.pos_encoding = nn.Parameter(torch.randn(1, d_model, height, width))

        # æ”¹è¿›ï¼šä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–
        self.pos_encoding = nn.Parameter(
            torch.randn(1, d_model, height, width) * 0.02  # âœ“ ç¼©å°åˆå§‹åŒ–
        )

    def forward(self, x):
        return x + self.pos_encoding


class ResidualQueryFusion(nn.Module):
    def __init__(self, d_model=256):
        super().__init__()
        # è½»é‡çº§çš„è‡ªé€‚åº”æ¨¡å—
        self.content_proj = nn.Linear(d_model, d_model, bias=False)
        self.position_proj = nn.Linear(d_model, d_model, bias=False)
        self.gamma = nn.Parameter(torch.zeros(1))  # å¯å­¦ä¹ çš„ç¼©æ”¾å› å­

    def forward(self, content, position):
        # åˆ†åˆ«æŠ•å½±
        content_proj = self.content_proj(content)
        position_proj = self.position_proj(position)

        # è‡ªé€‚åº”èåˆ
        fused = content_proj + position_proj

        # æ®‹å·®è¿æ¥ + å¯å­¦ä¹ ç¼©æ”¾
        output = content + self.gamma * fused

        return output