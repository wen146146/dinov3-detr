import math

import torch
import torch.nn as nn
# ç‰¹å¾å›¾å±•å¹³åä¸¢å¤±äº†ç©ºé—´ä½ç½®ä¿¡æ¯ï¼Œå¿…é¡»æ·»åŠ ä½ç½®ç¼–ç 
class SimpleDETRHead(nn.Module):
    def __init__(self, num_classes=80, num_queries=10,feat_height=14, feat_width=14):#è®¾ç½®å…«åä¸ªç±»åˆ«ï¼Œæœ€å¤šæŸ¥è¯¢100ä¸ªç›®æ ‡
        super().__init__()
        self.num_queries = num_queries

        # ç‰¹å¾æŠ•å½±
        #self.input_proj = nn.Conv2d(768, 256, kernel_size=1)
        #é™ç»´ï¼šå°†768ç»´ç‰¹å¾é™åˆ°256ç»´ï¼Œå‡å°‘è®¡ç®—é‡  å·ç§¯æ ¸: 1x1 - åªæ”¹å˜é€šé“æ•°ï¼Œä¸æ”¹å˜ç©ºé—´å°ºå¯¸
        self.input_proj = nn.Sequential(
            nn.Conv2d(768, 512, kernel_size=3, padding=1),  # 3Ã—3å·ç§¯
            nn.BatchNorm2d(512),# æ‰¹å½’ä¸€åŒ–
            nn.ReLU(inplace=True),# æ¿€æ´»å‡½æ•°
            nn.Conv2d(512, 256, kernel_size=1),  # å†ç”¨1Ã—1é™ç»´
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        # æŸ¥è¯¢å‘é‡
        # ğŸ”¥ ç¬¬1æ­¥ï¼šåˆ›å»º100å¼ åœ°å›¾ï¼Œå’Œ
        self.query_embed = nn.Embedding(num_queries, 256)#è¿™é‡Œçš„256å’Œä¸Šè¿°çš„ç‰¹å¾å€¼æ— å…³
        # ğŸ”¥ ç¬¬2æ­¥ï¼šç»™æ¯ä¸ªä¾¦æ¢åŸ¹è®­ä¸“ä¸šæŠ€èƒ½ï¼Œå‘Šè¯‰ä¾¦æ¢åº”è¯¥æ‰¾ä»€ä¹ˆ
        self.content_queries = nn.Embedding(num_queries, 256)
        #ç»™100ä¸ªä¾¦æ¢åˆ†é…ä»»åŠ¡
        self._init_queries(num_queries)
        #  æŸ¥è¯¢æ•°é‡: num_queries=100 - DETRæ ‡å‡†è®¾ç½®
        # æ¯ä¸ªæŸ¥è¯¢å¯¹åº”ä¸€ä¸ªå¯èƒ½çš„æ£€æµ‹ç»“æœ
        # 100ä¸ªæŸ¥è¯¢æœ€å¤šæ£€æµ‹100ä¸ªç›®æ ‡
        # ç»´åº¦: 256 - ä¸Transformeréšè—ç»´åº¦ä¸€è‡´
        # Transformerè§£ç å™¨ï¼ˆ1å±‚ç®€åŒ–ç‰ˆï¼‰
        decoder_layer = nn.TransformerDecoderLayer(d_model=256, # d_model=256: ç‰¹å¾ç»´åº¦
                                                   nhead=8,# nhead=8: æ³¨æ„åŠ›å¤´æ•°ï¼ˆ8ä¸ªå¤´å¹¶è¡Œå¤„ç†ï¼‰# æ¯ä¸ªå¤´ç»´åº¦ = 256 / 8 = 32(å…«ä¸ªç»´åº¦ï¼Œæ¯ä¸ªæ³¨æ„32ä¸ªç‰¹å¾ï¼Œå¯ä»¥æ›´ä¸“ä¸š)
                                                   batch_first=True,
                                                   dim_feedforward=2048,  # å¢åŠ FFNç»´åº¦FFNç»´åº¦=2048å°±æ˜¯å°†åŸæœ¬çš„256ä¸ªç‰¹å¾å€¼æ‰©å±•æˆ2048ä¸ªï¼Œç„¶åè¿›è¡Œåˆ†æï¼Œåˆ†æç»“æŸåå†è½¬å›256ï¼Œè¿™æ ·å¯ä»¥ä½¿å¾—åˆ°çš„256ä¸ªç‰¹å¾å€¼æ›´ç²¾å‡†
                                                   dropout=0.1  # æ·»åŠ dropoutå…³é—­0.1çš„ç¥ç»å…ƒé˜²æ­¢è¿‡æ‹Ÿåˆ
                                                   )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)
        # å±‚å½’ä¸€åŒ–ï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
        self.norm = nn.LayerNorm(256)
        # é¢„æµ‹å¤´
        # æ”¹ä¸ºï¼š
        self.class_head = nn.Sequential(
            nn.Linear(256, 256),  # ç¬¬ä¸€å±‚ï¼šç‰¹å¾æ·±åŒ–ï¼ˆå°†ç‰¹å¾å€¼ä¸­çš„ç‰¹å¾ç»„åˆï¼Œå’Œå»é™¤å™ªå£°ç­‰ï¼Œä½¿å¾—è¡¨ç°æ›´åŠ æ¸…æ™°ï¼‰
            nn.LayerNorm(256),  # å°†ç‰¹å¾å€¼ç¨³å®šå†ä¸€å®šèŒƒå›´ï¼Œç¨³å®šè®­ç»ƒ
            nn.ReLU(inplace=True),  #å°†è´Ÿç‰¹å¾å€¼å›ºå®šä¸º0 éçº¿æ€§æ¿€æ´»
            nn.Dropout(0.1),  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(256, num_classes + 1)  # æœ€ç»ˆåˆ†ç±»
        )

        self.bbox_head = nn.Sequential(
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(256, 4)  # [cx, cy, w, h]
        )

        self.pos_encoding = PositionalEncoding2D(256, feat_height, feat_width)

    def forward(self, feature_map):
        batch_size = feature_map.shape[0]

        # ç‰¹å¾æŠ•å½±
        memory = self.input_proj(feature_map)
        # è¾“å…¥: [batch, 768, H, W]
        # è¾“å‡º: [batch, 256, H, W]
        memory = self.pos_encoding(memory)  # ğŸ”¥ åœ¨è¿™é‡Œæ·»åŠ 2Dä½ç½®ç¼–ç 
        # memory: [batch, 256, H, W]ï¼Œå·²ç»åŒ…å«ä½ç½®ä¿¡æ¯
        # å‡è®¾è¾“å…¥: [2, 256, 14, 14]
        memory = (memory.flatten(2).  # [2, 256, 196]  # å°†14Ã—14=196ä¸ªç©ºé—´ä½ç½®å±•å¹³ï¼ˆåƒç´ åæ ‡ä¸¢å¤±ï¼Œéœ€è¦ä½ç½®ç¼–ç ï¼‰
                  permute(0, 2, 1))  # [2, 196, 256]  # äº¤æ¢ç»´åº¦

        # ğŸ”¥ 1. ä½ç½®æŸ¥è¯¢ï¼ˆåœ°å›¾ï¼‰
        query_pos = self.query_embed.weight.unsqueeze(0).repeat(batch_size, 1, 1)

        # ğŸ”¥ 2. å†…å®¹æŸ¥è¯¢ï¼ˆå¤§è„‘ï¼‰
        tgt = self.content_queries.weight.unsqueeze(0).repeat(batch_size, 1, 1)

        # âœ… æ­£ç¡®çš„è§£ç å™¨è°ƒç”¨ï¼ˆå°†ä½ç½®ä¿¡æ¯åŠ åˆ°å†…å®¹ä¸­ï¼‰ï¼š
        # æ–¹æ³•1ï¼šç›´æ¥ç›¸åŠ ï¼ˆæœ€ç®€å•ï¼‰
        tgt_with_pos = tgt + query_pos  # å°†ä½ç½®ä¿¡æ¯åŠ åˆ°å†…å®¹ä¸­
        # è§£ç 
        output = self.decoder(tgt_with_pos, memory)
        # output: [batch, 100, 256]  # æ›´æ–°åçš„æŸ¥è¯¢è¡¨ç¤º é€šè¿‡memoryçš„æ•°æ®è§£ç ï¼Œè·å¾—çš„tgt
        # æ¯å¼ å›¾åƒæœ‰100ä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢æœ‰256ç»´çš„è¡¨ç¤ºï¼Œè¿™ä¸ªè¡¨ç¤ºåŒ…å«äº†è¯¥æŸ¥è¯¢å…³æ³¨çš„ç‰©ä½“ä¿¡æ¯ï¼ˆæ­£å¸¸éœ€è¦å…­ä¸ªï¼‰
        # å±‚å½’ä¸€åŒ–
        output = self.norm(output)#å¹³è¯æƒé‡å€¼
        # é¢„æµ‹
        pred_logits = self.class_head(output)  # [batch, 100, 81]  å°†256ä¸ªç‰¹å¾å€¼ï¼Œè½¬åŒ–æˆ81ä¸ªç±»çš„ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”
        pred_boxes = torch.sigmoid(self.bbox_head(output))  # [batch, 100, 4] ç»™å‡ºè¿™100ä¸ªé¢„æµ‹çš„åæ ‡

        return pred_logits, pred_boxes  # è¿”å›é¢„æµ‹çš„ç±»åˆ«å’Œè¾¹ç•Œæ¡†

    def _init_queries(self, num_queries):
        """æ™ºèƒ½åˆå§‹åŒ–æŸ¥è¯¢å‘é‡"""
        # æ–¹æ³•1ï¼šæ›´å¥½çš„æƒé‡åˆå§‹åŒ–
        # nn.init.xavier_uniform_(self.query_embed.weight)  # ä½ç½®æŸ¥è¯¢åˆå§‹åŒ–
        # nn.init.xavier_uniform_(self.content_queries.weight)  # å†…å®¹æŸ¥è¯¢åˆå§‹åŒ–
        #ä»–ç¬¬ä¸€æ¬¡ç”Ÿæˆï¼Œæ˜¯æ¯”è¾ƒæœ‰ç»éªŒçš„åˆ†é…äº†ä¸€ä¸‹ï¼Œä»¥åå¯ä»¥è€ƒè‡ªå·±è®­ç»ƒ
        nn.init.uniform_(self.query_embed.weight, -1.0, 1.0)

        # å†…å®¹æŸ¥è¯¢ï¼ˆcontent_queriesï¼‰ï¼šå­¦ä¹ ç‰©ä½“ç‰¹å¾
        # ä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼Œæ ‡å‡†å·®0.02ï¼ˆå°å€¼é¿å…åˆå§‹æ¿€æ´»è¿‡å¤§ï¼‰
        nn.init.normal_(self.content_queries.weight, mean=0.0, std=0.02)


class PositionalEncoding2D(nn.Module):
    def __init__(self, d_model, height, width):
        super().__init__()
        if d_model % 4 != 0:
            d_model = (d_model // 4) * 4

        pe = torch.zeros(1, d_model, height, width)

        # æ¯ä¸ªä½ç½®ç¼–ç çš„ç»´åº¦ï¼ˆxå’Œyå„å ä¸€åŠï¼‰
        d_model_half = d_model // 2

        # ä¸ºé«˜åº¦å’Œå®½åº¦åˆ†åˆ«åˆ›å»ºä½ç½®ç¼–ç 
        pos_h = torch.arange(height).float().unsqueeze(1)  # [height, 1]
        pos_w = torch.arange(width).float().unsqueeze(0)  # [1, width]

        # ç”Ÿæˆä¸åŒé¢‘ç‡
        div_term = torch.exp(
            torch.arange(0, d_model_half, 2).float() *
            -(math.log(10000.0) / d_model_half)
        )  # [d_model_half/2]

        # é«˜åº¦ç¼–ç ï¼ˆå‰d_model_halfä¸ªé€šé“ï¼‰
        for i in range(0, d_model_half, 2):
            freq = div_term[i // 2]
            pe[0, i, :, :] = torch.sin(pos_h * freq).expand(-1, width)
            pe[0, i + 1, :, :] = torch.cos(pos_h * freq).expand(-1, width)

        # å®½åº¦ç¼–ç ï¼ˆåd_model_halfä¸ªé€šé“ï¼‰
        for i in range(0, d_model_half, 2):
            freq = div_term[i // 2]
            pe[0, d_model_half + i, :, :] = torch.sin(pos_w * freq).expand(height, -1)
            pe[0, d_model_half + i + 1, :, :] = torch.cos(pos_w * freq).expand(height, -1)

        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe
